x-airflow-common:
    &airflow-common
    build: .
    environment:
        &airflow-common-env
        AIRFLOW__CORE__EXECUTOR: CeleryExecutor
#        AIRFLOW__CORE__EXECUTOR: SequentialExecutor
        AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
        AIRFLOW__API_AUTH__JWT_SECRET: 5bc9ac170e7504b793bd7e4c75a36c1cf87c69a5e6c99d51fb0af58890ecc5f4
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
        AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
        AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
        AIRFLOW__CORE__FERNET_KEY: ''
        AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
        AIRFLOW__LOGGING__LOGGING_LEVEL: WARNING
#        AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
        AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'
        AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
        AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR: 'true'
        AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
    volumes:
        - ./dags:/opt/airflow/dags
        - ./logs:/opt/airflow/logs
        - ./config:/opt/airflow/config
        - ./engineering:/opt/airflow/engineering
        - ./requirements.txt:/opt/airflow/requirements.txt
    user: "50000:0"
    depends_on:
        &airflow-common-depends-on
        redis:
            condition: service_healthy
        postgres:
            condition: service_healthy

services:
    postgres:
        image: postgres:13
        environment:
            POSTGRES_USER: airflow
            POSTGRES_PASSWORD: airflow
            POSTGRES_DB: airflow
        volumes:
            - postgres-db-volume:/var/lib/postgresql/data
        healthcheck:
            test: [ "CMD", "pg_isready", "-U", "airflow" ]
            interval: 5s
            retries: 5
            start_period: 5s

    redis:
        image: redis:7.2-bookworm
        expose:
          - 6379
        healthcheck:
          test: ["CMD", "redis-cli", "ping"]
          interval: 10s
          timeout: 30s
          retries: 50
          start_period: 30s
        restart: always
    
    airflow-apiserver:
        <<: *airflow-common
        command: api-server
        ports:
            - '8080:8080'
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://127.0.0.1:8080/api/v2/version"]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: always
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
    
    airflow-scheduler:
        <<: *airflow-common
        command: scheduler
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://127.0.0.1:8080/api/v2/version"]
            interval: 30s
            timeout: 10s
            retries: 5
            start_period: 30s
        restart: on-failure
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
                
    airflow-dag-processor:
        <<: *airflow-common
        command: dag-processor
        restart: on-failure
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
    
    airflow-worker:
        <<: *airflow-common
        command: celery worker
        healthcheck:
          test:
            - "CMD-SHELL"
            - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
          interval: 30s
          timeout: 10s
          retries: 5
          start_period: 30s
        environment:
          <<: *airflow-common-env
          DUMB_INIT_SETSID: "0"
        restart: on-failure
        depends_on:
          <<: *airflow-common-depends-on
          airflow-apiserver:
            condition: service_healthy
          airflow-init:
            condition: service_completed_successfully
    
    airflow-triggerer:
        <<: *airflow-common
        command: triggerer
        healthcheck:
          test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
          interval: 30s
          timeout: 10s
          retries: 5
          start_period: 30s
        restart: on-failure
        depends_on:
          <<: *airflow-common-depends-on
          airflow-init:
            condition: service_completed_successfully
    
    airflow-init:
        <<: *airflow-common
        entrypoint: /bin/bash
        command:
            - -c
            - |
                echo "Creating missing opt dirs if missing:"
                echo
                mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
                echo
                echo "Airflow version:"
                /entrypoint airflow version
                echo
                echo "Files in shared volumes:"
                echo
                ls -la /opt/airflow/{logs,dags,plugins,config}
        environment:
            <<: *airflow-common-env
            _AIRFLOW_DB_MIGRATE: 'true'
            _AIRFLOW_WWW_USER_CREATE: 'true'
            _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
            _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
            _PIP_ADDITIONAL_REQUIREMENTS: ''
        user: "0:0"
        depends_on:
            <<: *airflow-common-depends-on
        
    airflow-cli:
        <<: *airflow-common
        profiles:
        - debug
        environment:
            <<: *airflow-common-env
            CONNECTION_CHECK_MAX_COUNT: "0"
        # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
        command:
        - bash
        - -c
        - airflow
        depends_on:
            <<: *airflow-common-depends-on
            airflow-init:
                condition: service_completed_successfully
        
volumes:
    postgres-db-volume:
